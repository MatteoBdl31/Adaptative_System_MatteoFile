#!/usr/bin/env python3
"""
Script d'analyse robuste pour le LOT 7
- D√©tecte les r√®gles CSS mortes (non utilis√©es)
- Analyse la sp√©cificit√© excessive
- Identifie les duplications
"""

import re
import os
from pathlib import Path
from collections import defaultdict
from typing import Set, Dict, List, Tuple

# Configuration
PROJECT_ROOT = Path(__file__).parent
CSS_FILE = PROJECT_ROOT / "adaptive_quiz_system/static/style.css"
TEMPLATES_DIR = PROJECT_ROOT / "adaptive_quiz_system/templates"
STATIC_DIR = PROJECT_ROOT / "adaptive_quiz_system/static"
BACKEND_DIR = PROJECT_ROOT / "adaptive_quiz_system"

# Classes Leaflet √† ne jamais consid√©rer comme mortes
LEAFLET_CLASSES = {
    'leaflet-container', 'leaflet-map-pane', 'leaflet-tile-pane', 
    'leaflet-tile', 'leaflet-popup-content-wrapper', 'leaflet-popup-content',
    'leaflet-control', 'leaflet-control-zoom', 'leaflet-control-layers'
}

# Classes g√©n√©r√©es dynamiquement par JS (√† v√©rifier manuellement)
DYNAMIC_CLASSES = {
    'active', 'hidden', 'loading', 'disabled', 'selected', 'open', 'closed'
}

def extract_css_selectors(css_content: str) -> Tuple[Set[str], Set[str], Dict[str, int]]:
    """Extrait tous les s√©lecteurs CSS du fichier"""
    classes = set()
    ids = set()
    specificity_map = {}  # classe -> sp√©cificit√© estim√©e
    
    # Pattern pour les classes (plus robuste)
    class_pattern = r'\.([a-zA-Z0-9_-]+)(?::[a-zA-Z0-9_-]+)?(?:\s|,|{|\.|#)'
    
    # Pattern pour les IDs
    id_pattern = r'#([a-zA-Z0-9_-]+)'
    
    # Extraire les classes
    for match in re.finditer(class_pattern, css_content):
        class_name = match.group(1)
        # Ignorer les pseudo-classes communes
        if class_name not in ['hover', 'focus', 'active', 'before', 'after', 
                              'first', 'last', 'nth', 'not', 'is', 'where']:
            classes.add(class_name)
    
    # Extraire les IDs
    for match in re.finditer(id_pattern, css_content):
        ids.add(match.group(1))
    
    # Calculer la sp√©cificit√© (simplifi√©: compter les s√©lecteurs dans la cha√Æne)
    lines = css_content.split('\n')
    current_selector = None
    for line in lines:
        line = line.strip()
        if not line or line.startswith('/*') or line.startswith('*'):
            continue
        
        # D√©tecter un s√©lecteur (ligne qui se termine par {)
        if '{' in line and not line.startswith('@'):
            selector = line.split('{')[0].strip()
            # Compter la sp√©cificit√© (nombre de classes/IDs dans le s√©lecteur)
            specificity = (
                len(re.findall(r'#\w+', selector)) * 100 +  # IDs
                len(re.findall(r'\.\w+', selector)) * 10 +  # Classes
                len(re.findall(r'\w+(?=\s|$)', selector))   # √âl√©ments
            )
            
            # Extraire les classes du s√©lecteur
            for class_match in re.finditer(r'\.([a-zA-Z0-9_-]+)', selector):
                class_name = class_match.group(1)
                if class_name not in specificity_map or specificity_map[class_name] < specificity:
                    specificity_map[class_name] = specificity
    
    return classes, ids, specificity_map

def search_in_files(selectors: Set[str], file_paths: List[Path], file_type: str = "html") -> Dict[str, List[str]]:
    """Recherche l'usage des s√©lecteurs dans les fichiers"""
    usage_map = defaultdict(list)
    
    for file_path in file_paths:
        if not file_path.exists():
            continue
        
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            
            for selector in selectors:
                # Recherche robuste
                patterns = [
                    f'class="{selector}"',  # class="selector"
                    f'class=\'{selector}\'',  # class='selector'
                    f'class="[^"]*\\b{selector}\\b',  # dans une liste de classes
                    f'class=\'[^\']*\\b{selector}\\b',  # dans une liste de classes
                    f'getElementById\\(["\']{selector}["\']\\)',  # JS: getElementById
                    f'querySelector\\(["\'][^"\']*{selector}',  # JS: querySelector
                    f'querySelectorAll\\(["\'][^"\']*{selector}',  # JS: querySelectorAll
                    f'\\.{selector}\\b',  # JS: .selector
                    f'#{selector}\\b',  # JS: #selector
                    f'classList\\.(add|remove|toggle|contains)\\(["\']{selector}["\']\\)',  # JS: classList
                ]
                
                for pattern in patterns:
                    if re.search(pattern, content, re.IGNORECASE):
                        usage_map[selector].append(str(file_path))
                        break
        except Exception as e:
            print(f"Erreur lecture {file_path}: {e}")
    
    return usage_map

def find_duplicate_rules(css_content: str) -> List[Tuple[str, List[str]]]:
    """Trouve les r√®gles CSS dupliqu√©es (strictement identiques)"""
    # Extraire toutes les r√®gles CSS
    rule_pattern = r'([^{]+)\{([^}]+)\}'
    rules = {}
    duplicates = []
    
    for match in re.finditer(rule_pattern, css_content, re.MULTILINE | re.DOTALL):
        selector = match.group(1).strip()
        declarations = match.group(2).strip()
        
        # Normaliser (supprimer espaces, commentaires)
        normalized_decls = re.sub(r'/\*.*?\*/', '', declarations, flags=re.DOTALL)
        normalized_decls = re.sub(r'\s+', ' ', normalized_decls).strip()
        
        if normalized_decls:
            key = normalized_decls
            if key in rules:
                if selector not in rules[key]:
                    rules[key].append(selector)
                    duplicates.append((key, rules[key]))
            else:
                rules[key] = [selector]
    
    return duplicates

def analyze_specificity(specificity_map: Dict[str, int]) -> List[Tuple[str, int]]:
    """Identifie les s√©lecteurs avec sp√©cificit√© excessive (> 50)"""
    high_specificity = [(cls, spec) for cls, spec in specificity_map.items() if spec > 50]
    return sorted(high_specificity, key=lambda x: x[1], reverse=True)

def main():
    print("=" * 80)
    print("ANALYSE LOT 7 - D√©tection r√®gles mortes, sp√©cificit√©, duplications")
    print("=" * 80)
    
    # 1. Lire le CSS
    print("\n1. Lecture du fichier CSS...")
    css_content = CSS_FILE.read_text()
    print(f"   ‚úì Fichier CSS lu: {len(css_content)} caract√®res")
    
    # 2. Extraire les s√©lecteurs
    print("\n2. Extraction des s√©lecteurs CSS...")
    classes, ids, specificity_map = extract_css_selectors(css_content)
    print(f"   ‚úì Classes trouv√©es: {len(classes)}")
    print(f"   ‚úì IDs trouv√©s: {len(ids)}")
    
    # 3. Trouver tous les fichiers √† analyser
    print("\n3. Recherche des fichiers √† analyser...")
    html_files = list(TEMPLATES_DIR.rglob("*.html"))
    js_files = list(STATIC_DIR.rglob("*.js"))
    py_files = list(BACKEND_DIR.rglob("*.py"))
    
    print(f"   ‚úì Templates HTML: {len(html_files)}")
    print(f"   ‚úì Fichiers JS: {len(js_files)}")
    print(f"   ‚úì Fichiers Python: {len(py_files)}")
    
    # 4. Rechercher l'usage des classes
    print("\n4. Recherche de l'usage des classes CSS...")
    all_files = html_files + js_files
    class_usage = search_in_files(classes, all_files, "html")
    
    # 5. Identifier les r√®gles mortes
    print("\n5. Identification des r√®gles mortes...")
    unused_classes = set(classes) - set(class_usage.keys())
    
    # Exclure les classes Leaflet et dynamiques
    unused_classes = unused_classes - LEAFLET_CLASSES
    unused_classes = unused_classes - {c for c in unused_classes if any(dc in c.lower() for dc in DYNAMIC_CLASSES)}
    
    # Exclure les nouvelles classes cr√©√©es dans les LOTs (c-*, l-*, u-*)
    unused_classes = unused_classes - {c for c in unused_classes if c.startswith(('c-', 'l-', 'u-'))}
    
    print(f"   ‚úì Classes utilis√©es: {len(class_usage)}")
    print(f"   ‚úì Classes potentiellement inutilis√©es: {len(unused_classes)}")
    
    # 6. Analyser la sp√©cificit√©
    print("\n6. Analyse de la sp√©cificit√©...")
    high_specificity = analyze_specificity(specificity_map)
    print(f"   ‚úì S√©lecteurs avec sp√©cificit√© > 50: {len(high_specificity)}")
    
    # 7. Trouver les duplications
    print("\n7. Recherche des r√®gles dupliqu√©es...")
    duplicates = find_duplicate_rules(css_content)
    print(f"   ‚úì R√®gles dupliqu√©es trouv√©es: {len(duplicates)}")
    
    # 8. G√©n√©rer le rapport
    print("\n" + "=" * 80)
    print("RAPPORT D'ANALYSE")
    print("=" * 80)
    
    print(f"\nüìä STATISTIQUES:")
    print(f"   - Classes CSS: {len(classes)}")
    print(f"   - IDs CSS: {len(ids)}")
    print(f"   - Classes utilis√©es: {len(class_usage)}")
    print(f"   - Classes potentiellement inutilis√©es: {len(unused_classes)}")
    print(f"   - S√©lecteurs haute sp√©cificit√© (>50): {len(high_specificity)}")
    print(f"   - R√®gles dupliqu√©es: {len(duplicates)}")
    
    if unused_classes:
        print(f"\n‚ö†Ô∏è  CLASSES POTENTIELLEMENT INUTILIS√âES (premiers 30):")
        for i, cls in enumerate(sorted(list(unused_classes))[:30], 1):
            print(f"   {i:2d}. .{cls}")
        if len(unused_classes) > 30:
            print(f"   ... et {len(unused_classes) - 30} autres")
    
    if high_specificity:
        print(f"\n‚ö†Ô∏è  S√âLECTEURS HAUTE SP√âCIFICIT√â (premiers 20):")
        for i, (cls, spec) in enumerate(high_specificity[:20], 1):
            print(f"   {i:2d}. .{cls} (sp√©cificit√©: {spec})")
    
    if duplicates:
        print(f"\n‚ö†Ô∏è  R√àGLES DUPLIQU√âES (premiers 10):")
        for i, (decls, selectors) in enumerate(duplicates[:10], 1):
            print(f"   {i:2d}. {len(selectors)} s√©lecteurs avec m√™mes d√©clarations:")
            for sel in selectors[:3]:
                print(f"       - {sel}")
            if len(selectors) > 3:
                print(f"       ... et {len(selectors) - 3} autres")
    
    print("\n" + "=" * 80)
    print("Analyse termin√©e. V√©rifiez manuellement les r√©sultats avant suppression.")
    print("=" * 80)

if __name__ == "__main__":
    main()
